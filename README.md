# DNN 딥러닝 회귀분석 프로젝트

이 프로젝트는 PyTorch를 사용하여 전력 사용량 예측을 위한 DNN 회귀 분석을 수행합니다. <br>
## 🗂️ 0. 프로젝트 흐름도
다음은 이 프로젝트의 전반적인 흐름도입니다.
![image](https://github.com/user-attachments/assets/43958a0b-8708-47c8-91f1-304b8a4ae3e4)


## 📊 1. 데이터 전처리 및 시각화

1. **데이터 로드 및 전처리**:
   - CSV 파일로부터 데이터를 로드합니다.
   - 'date_time' 열을 날짜 차이로 변환하여 숫자 형태로 변환합니다.

2. **상자 그림(Box Plot)**:
   - box plot을 통해 이상치를 파악해보고자, 숫자형 컬럼에 대해 상자 그림을 통해 데이터 분포를 시각화합니다.
   - ![image](https://github.com/user-attachments/assets/5f6ccedc-e782-420e-aeba-434a76a3ab7f)
   - 그러나,,, boxplot으로 확인해본 결과, 단순 그래프 기반으로 이상치를 판단하기에는 도메인 지식이 부족해서, 전 데이터 바탕으로 분석을 진행했습니다.
     
3. **상관관계 분석**:
   - 변수 간 상관관계를 pairplot과 히트맵으로 시각화하여 상관관계를 파악합니다.
   - ![image](https://github.com/user-attachments/assets/852c074f-b822-4325-9bec-2fdd27de7032)

4. **KMeans 클러스터링**:
   - KMeans 클러스터링을 통해 데이터의 군집화를 수행하고 클러스터 레이블을 파생 변수로 추가합니다.

## 🧠 2. 모델링 및 학습

1. **모델 정의**:
   - PyTorch를 사용하여 심층 신경망(DNN) 모델을 정의합니다.
   - 모델에는 여러 개의 은닉층과 배치 정규화가 포함되어 있습니다.

2. **학습 및 평가**:
   - K-Fold Cross Validation을 사용하여 모델의 일반화 성능을 평가합니다.
   - Early Stopping을 적용하여 성능 개선이 없는 경우 훈련을 중단합니다.

3. **하이퍼파라미터 설정**:
   - 학습률, 배치 크기, 에포크 수 등 하이퍼파라미터를 설정합니다.

## 📈3.  결과

- **평균 평가 손실**: 모델의 최종 성능을 평가하기 위해 평균 평가 손실을 계산합니다.


## 🔥 + 성능 향상을 위한 설정 변경

1. **스탠다드 스케일링 사용**:
   - DNN 모델은 데이터의 스케일에 민감하므로, 스탠다드 스케일링을 적용하여 성능을 향상시켰습니다.
   - 로버스트 스케일링과 고민했지만, 로버스트는 **사분위 범위(IQR)**를 사용하여 데이터의 분포를 조정하기 때문에, 데이터의 원래 분포 형태가 왜곡될 수 있어서 스탠다드 스케일링으로 진행했습니다.
   - *Min-Max 스케일링은 이상치에 크게 민감하게 반응해, 아예 고려도 하지 않았어요. Min-Max 스케일링은 주로 이미지 같은 부분에서 활용한다고...(0~255 픽셀 값이 정해져 있어서 이상치가 존재하지 않기 때문)*


2. **은닉층 한 층 제거 (모델 단순화)**:
   - 모델의 복잡도를 줄여서 일반화 성능을 개선했습니다. (모델 단순화가 항상 손실 감소를 보장하지는 않아요!)

3. **드롭아웃 설정 제거**:
   - 드롭아웃 비율을 0.5로 설정한 것을 0.2로 다운해서 설정해보았다가, 결국 주석 처리했습니다. 더 많은 학습이 필요하다고 판단되었기 때문입니다.

4. **학습률 조정**:
   - `learning_rate`를 0.001에서 0.01로 변경했습니다. 학습 속도를 높이기 위해서지만, 너무 큰 값은 학습의 안정성을 해칠 수 있다는 점!!

5. **학습률 감소 주기 변경**:
   - `step_size`를 2에서 4로 변경하여 4 에포크마다 학습률이 감소하도록 했습니다. 이로 인해 손실이 줄어들었습니다.

6. **배치 크기 조정**:
   - `batch_size`를 64에서 512로 변경하여 학습 속도를 증가시켰습니다.
